{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "this notebook accompanies the work \"The Growing N-Gram Algorithm: A Novel Approach to String Clustering\" by Corrado Grappiolo, Eline Verwielen and Nils Noorman, in Proceedings of the 8th International Conference on Pattern Recognition Applications and Methods (ICPRAM19, http://www.icpram.org/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nGram_recursiveKneserNey import recursive_NGramKneserNey as KN\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(\"../data./xRaySequenceDataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['train', 'test', 'validation', 'total']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasetDescr = {l : {} for l in labels}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for l in labels :\n",
    "    \n",
    "    if l == 'total' :\n",
    "        df = dataset.copy()\n",
    "    else :\n",
    "        df = dataset[dataset['type'] == l].copy()\n",
    "    \n",
    "    datasetDescr[l]['Size'] = len(df)\n",
    "    datasetDescr[l]['Unique Strings'] = len(df['sequence'].drop_duplicates())\n",
    "    \n",
    "    if l not in ['train', 'total'] :\n",
    "        datasetDescr[l]['String Intersection with Training Set'] = len(set(dataset[dataset['type'] == 'train']['sequence']).intersection(set(df['sequence'])))\n",
    "    \n",
    "    df['strLen'] = df['sequence'].apply(lambda x : len(x.split('-')))\n",
    "    symbols = [x.split('-') for x in list(df['sequence'])]\n",
    "    alphabeth = set([item for sublist in symbols for item in sublist])\n",
    "    \n",
    "    datasetDescr[l]['Alphabeth Size'] = len(alphabeth)\n",
    "    \n",
    "    if l not in ['train', 'total'] :\n",
    "        df_train = dataset[dataset['type'] == 'train']\n",
    "        symbols_train = [x.split('-') for x in list(df_train['sequence'])]\n",
    "        alphabeth_train = set([item for sublist in symbols_train for item in sublist])\n",
    "        datasetDescr[l]['Alphabeth Intersection with Training Set'] = len(alphabeth.intersection(alphabeth_train))\n",
    "    \n",
    "    datasetDescr[l]['Avg. String Length'] = np.round(np.mean(df['strLen']), 2)\n",
    "    datasetDescr[l]['Std. Dev. String Length'] = np.round(np.std(df['strLen']), 2)\n",
    "    datasetDescr[l]['Median String Length'] = np.round(np.median(df['strLen']), 2)\n",
    "    datasetDescr[l]['Min. String Length'] = np.round(np.min(df['strLen']), 2)\n",
    "    datasetDescr[l]['Max. String Length'] = np.round(np.max(df['strLen']), 2)\n",
    "    \n",
    "    #break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for l in labels :\n",
    "    print(l)\n",
    "    for k in datasetDescr[l] :\n",
    "        print('\\t', k, datasetDescr[l][k])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Growing N-Grams Algorithm Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def growing_n_gram_algorithm(inputTrainingSet, \n",
    "                             tau_c_init, tau_u_init,\n",
    "                             tau_c_delta = 1, tau_u_delta = 0.5,\n",
    "                             tau_c_min = 1, tau_u_max = 1,\n",
    "                             nGramValue = 2, dataColumnName = 'sequence',\n",
    "                             printProgress = False) :\n",
    "    \n",
    "    trainingSet = inputTrainingSet.copy()\n",
    "    trainingSet['fedToModels'] = False\n",
    "    trainingSet.reset_index(inplace=True, drop=True)\n",
    "    \n",
    "    tau_c = tau_c_init\n",
    "    tau_u = tau_u_init\n",
    "    \n",
    "    nGramDict = {}\n",
    "    \n",
    "    loopIndex = 0\n",
    "    loop = True\n",
    "    \n",
    "    if printProgress :\n",
    "        print(\"start growing ngram settings:\")\n",
    "        print(\"    tau_c_init:\", tau_c_init)\n",
    "        print(\"    tau_c_delta:\", tau_c_delta)\n",
    "        print(\"    tau_c_min:\", tau_c_min)\n",
    "        print(\"    tau_u_init:\", tau_u_init)\n",
    "        print(\"    tau_u_delta:\", tau_u_delta)\n",
    "        print(\"    tau_u_max:\", tau_u_max)\n",
    "        print(\"    n:\", nGramValue)\n",
    "      \n",
    "    while loop :\n",
    "        \n",
    "        trainingSet = trainingSet[trainingSet['fedToModels'] == False]\n",
    "        if len(trainingSet) == 0 :\n",
    "            loop = False\n",
    "            continue\n",
    "        trainingSet = trainingSet.sample(frac = 1)\n",
    "        trainingSet.reset_index(inplace=True, drop=True)\n",
    "\n",
    "        nFed = 0\n",
    "        nSkip = 0\n",
    "        nCreate = 0\n",
    "        nUpdate = 0\n",
    "\n",
    "        if printProgress :\n",
    "            print(\"loop =\", loopIndex, \n",
    "                  \", toFeed =\", len(trainingSet), \n",
    "                  \", tau_create =\", tau_c, \n",
    "                  \", tau_update =\", tau_u,\n",
    "                  \", ensemble size =\", len(nGramDict)) \n",
    "\n",
    "        for x in range(len(trainingSet)) :\n",
    "            inputString = trainingSet.iloc[x][dataColumnName]\n",
    "\n",
    "            inputStringSet = set(inputString.split('-'))\n",
    "\n",
    "            M_c = []\n",
    "            M_u = []\n",
    "\n",
    "            for m in nGramDict :\n",
    "                model_unigrams = set([k[0] for k in nGramDict[m].grams[1]])\n",
    "                if len(inputStringSet.difference(model_unigrams)) > tau_c :\n",
    "                    M_c.append(m)\n",
    "\n",
    "                if len(inputStringSet.difference(model_unigrams)) <= tau_u :\n",
    "                    M_u.append(m)\n",
    "\n",
    "            if len(M_c) == len(nGramDict) :\n",
    "                # creation\n",
    "                newNgramIndex = len(nGramDict)\n",
    "                nGramDict[newNgramIndex] = KN(\"ID_\" + str(newNgramIndex) + \"_n_\" + str(nGramValue), \n",
    "                                              n=nGramValue, \n",
    "                                              useFixedVocabularySize=True, \n",
    "                                              fullVocabularySize=datasetDescr['total']['Alphabeth Size'] + 3,\n",
    "                                              modelOOVfromTrainingData=False, \n",
    "                                              printTodo=False)\n",
    "\n",
    "                nGramDict[newNgramIndex].feed(inputString)\n",
    "                trainingSet.at[x, 'fedToModels'] = True\n",
    "                nFed += 1\n",
    "                nCreate += 1\n",
    "\n",
    "            elif len(M_u) > 0 :\n",
    "                # update\n",
    "                probs = [nGramDict[m].chainProbability(inputString) for m in M_u]\n",
    "                winnerModel = M_u[np.argmax(probs)]\n",
    "                nGramDict[winnerModel].feed(inputString)\n",
    "                trainingSet.at[x, 'fedToModels'] =  True\n",
    "                nFed += 1\n",
    "                nUpdate += 1\n",
    "\n",
    "            else :\n",
    "                # just skip the inputString, hence do nothing, do not update 'fedToModels\n",
    "                nSkip += 1 \n",
    "\n",
    "\n",
    "        # end of feeding pass, parameter update\n",
    "        tau_u += tau_u_delta\n",
    "        if tau_u > tau_u_max :\n",
    "            tau_u = tau_u_max\n",
    "\n",
    "        tau_c -= tau_c_delta\n",
    "        if tau_c < tau_c_min :\n",
    "            tau_c = tau_c_min\n",
    "\n",
    "        loopIndex += 1\n",
    "    \n",
    "    if printProgress :\n",
    "        print(\"loop =\", loopIndex, \n",
    "              \", toFeed =\", len(trainingSet), \n",
    "              \", tau_create =\", tau_c, \n",
    "              \", tau_update =\", tau_u,\n",
    "              \", ensemble size =\", len(nGramDict))\n",
    "            \n",
    "    return nGramDict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Growing N-Grams Algorithm Execution, Entropy Calculation, Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingSet = dataset[dataset['type'] == 'train']\n",
    "validationSet = dataset[dataset['type'] == 'validation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nRuns = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensembles = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for r in range(nRuns) :\n",
    "    # GNG algorithm execution\n",
    "    print(\"start GNG run \", r)\n",
    "    nGramDict = growing_n_gram_algorithm(trainingSet, 20, -1, printProgress=True)\n",
    "    ensembles[r] = {'ensemble' : nGramDict}\n",
    "    \n",
    "    # entropy calculation on the validation set\n",
    "    ensembleH = []\n",
    "    for i in range(len(validationSet)) :\n",
    "        x = validationSet.iloc[i]['sequence']\n",
    "        chainProbs = [nGramDict[k].chainProbability(x) for k in nGramDict]\n",
    "        ensembleH.append(entropy(chainProbs))\n",
    "    ensembles[r]['validation_H'] = ensembleH\n",
    "    print(\"Avg. Validation Entropy:\", np.mean(ensembles[r]['validation_H']))\n",
    "    print(\"Std. Validation Entropy:\", np.std(ensembles[r]['validation_H']))\n",
    "    print(20*'-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "globalAvg = np.mean([ensembles[r]['validation_H'] for r in range(nRuns)])\n",
    "globalStd = np.std([ensembles[r]['validation_H'] for r in range(nRuns)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "globalEnselmbleSizeAvg = np.mean([len(ensembles[r]['ensemble']) for r in range(nRuns)])\n",
    "globalEnselmbleSizeStd = np.std([len(ensembles[r]['ensemble']) for r in range(nRuns)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Global Avg. Entropy:\", np.round(globalAvg, 4))\n",
    "print(\"Global Std. Entropy:\", np.round(globalStd, 4))\n",
    "print(\"Global Avg. Enselmble Size:\", np.round(globalEnselmbleSizeAvg, 4))\n",
    "print(\"Global Std. Ensemble Size:\", np.round(globalEnselmbleSizeStd, 4))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
